<tool id="tl.tsne" name="tl.tsne" version="1.3.1+galaxy1">
    <description>t-distributed stochastic neighborhood embedding (tSNE)</description>
    <macros>
        <import>macros.xml</import>
    </macros>
    <expand macro="requirements"/>
    <command detect_errors="exit_code"><![CDATA[
            python $script_file
    ]]></command>
    <configfiles>
        <configfile name="script_file"><![CDATA[
@CMD_imports@
@CMD_read_inputs@

sc.tl.tsne(
    adata=adata,
    #if $n_pcs
    n_pcs=$n_pcs,
    #end if
    perplexity=$perplexity,
    early_exaggeration=$early_exaggeration,
    learning_rate=$learning_rate,
    random_state=$random_state,
    copy=False)

@CMD_anndata_write_outputs@
np.savetxt('$X_tsne', adata.obsm['X_tsne'], delimiter='\t')
]]></configfile>
    </configfiles>
    <inputs>
        <expand macro="inputs_anndata"/>
        <param name="n_pcs" type="integer" value="" optional="true" label="Number of PCs to use" help=""/>
        <param name="perplexity" type="float" value="30" label="Perplexity" help="The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter."/>
        <param name="early_exaggeration" type="float" value="12.0" label="Early exaggeration" help="Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high."/>
        <param name="learning_rate" type="float" value="1000" label="Learning rate" help="The learning rate can be a critical parameter. It should be between 100 and 1000. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high. If the cost function gets stuck in a bad local minimum increasing the learning rate helps sometimes."/>
        <param name="random_state" type="integer" value="0" label="Random state" help="Change this to use different intial states for the optimization"/>
        <expand macro="anndata_output_format"/>
    </inputs>
    <outputs>
        <expand macro="anndata_outputs"/>
        <data name="X_tsne" format="tabular" label="${tool.name} on ${on_string}: tSNE coordinates"/>
    </outputs>
    <tests>
        <test>
            <conditional name="input">
                <param name="format" value="h5ad" />
                <param name="adata" value="krumsiek11.h5ad" />
            </conditional>
            <param name="n_pcs" value="10"/>
            <param name="perplexity" value="30"/>
            <param name="early_exaggeration" value="12.0"/>
            <param name="learning_rate" value="1000"/>
            <param name="random_state" value="0"/>
            <param name="anndata_output_format" value="h5ad" />
            <output name="anndata_out_h5ad" file="tl.tsne.krumsiek11.h5ad" ftype="h5" compare="sim_size"/>
            <output name="X_tsne" file="tl.tsne.krumsiek11_X_tsne.tabular"/>
        </test>
    </tests>
    <help><![CDATA[
t-distributed stochastic neighborhood embedding (tSNE) (Maaten et al, 2008) has been
proposed for visualizating single-cell data by (Amir et al, 2013). Here, by default,
we use the implementation of *scikit-learn* (Pedregosa et al, 2011). 


Return
------

X_tsne : tSNE coordinates of data.

More details on the `scanpy documentation
<https://scanpy.readthedocs.io/en/latest/api/scanpy.api.tl.tsne.html#scanpy.api.tl.tsne>`_

    ]]></help>
    <expand macro="citations"/>
</tool>
